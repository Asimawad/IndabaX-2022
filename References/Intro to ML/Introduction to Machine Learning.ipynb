{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbkbBAfI7Rfr"
   },
   "source": [
    "# Machine Learning in Python \n",
    "\n",
    "## Module 1: Mining and visualising real-world data\n",
    "### Learning Activity: Loading the Python libraries\n",
    "\n",
    "First you need to load the required Python libraries. Libraries are extensions to the base python that add functionality or help to make tasks more convenient to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "LOnPtxfA7Rft",
    "outputId": "dc077829-5016-4391-9ab6-be8493b87d7f"
   },
   "outputs": [],
   "source": [
    "# compatibility with python2 and 3\n",
    "from __future__ import print_function, division\n",
    "from __future__ import absolute_import \n",
    "\n",
    "# numerical capacity\n",
    "import scipy as scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib setup\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plotly setup\n",
    "import plotly \n",
    "from plotly.graph_objs import *\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "\n",
    "# extra tools\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D \n",
    "\n",
    "# GENERAL SKLEARN TOOLS\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# DTS and RFS MODULE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM MODULE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71TWTOnT7Rfy"
   },
   "source": [
    "# The dataset\n",
    "\n",
    "Financial Inclusion remains one of the main obstacles to economic and human development in Africa. For example, across Kenya, Rwanda, Tanzania, and Uganda only 9.1 million adults (or 13.9% of the adult population) have access to or use a commercial bank account.\n",
    "\n",
    "Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion. Despite the proliferation of mobile money in Africa, and the growth of innovative fintech solutions, banks still play a pivotal role in facilitating access to financial services. Access to bank accounts enable households to save and facilitate payments while also helping businesses build up their credit-worthiness and improve their access to other finance services. Therefore, access to bank accounts is an essential contributor to long-term economic growth.\n",
    "\n",
    "The objective of this competition is to create a machine learning model to predict which individuals are most likely to have or use a bank account. The models and solutions developed can provide an indication of the state of financial inclusion in Kenya, Rwanda, Tanzania and Uganda, while providing insights into some of the key demographic factors that might drive individuals’ financial outcomes.\n",
    "\n",
    "The dataset contains demographic information and what financial services are used by approximately 33,610 individuals across East Africa. This data was extracted from various Finscope surveys ranging from 2016 to 2018, You are asked to make predictions for each unique id in the test dataset about the likelihood of the person having a bank account.\n",
    "\n",
    "### Learning Activity: Importing the data\n",
    "\n",
    "As a first step we load the dataset from the provided `Train_v2.csv` file with `pandas`. To achieve this you will use the `.read_csv()` method from Pandas. We just need to point to the location of the dataset and indicate under what name we want to store the data, i.e. `df`. \n",
    "\n",
    "Once the data has been loaded, you can look at the first few instances using the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "VE5W51Ps7Rfz",
    "outputId": "55969aa7-0039-4770-df80-4fe1fde0c2af"
   },
   "outputs": [],
   "source": [
    "# Import the data and explore the first few rows\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/StarBoy01/IndabaX-Sudan-2019/master/Train_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "hKzsafuw7Rf2",
    "outputId": "140167e3-2fc9-4255-f6af-1c0d500bcf52"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "O9jep9iP7Rf4",
    "outputId": "e4bc6021-f010-453a-9e7c-534eb772e960"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "vW8395bn7Rf6",
    "outputId": "87158b66-e897-4fe8-cc83-46b19aefad99"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhzfiE-Z7Rf9"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6WTLayh7Rf-"
   },
   "source": [
    "## dataset balance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZapgbim7Rf_"
   },
   "source": [
    "Lets see if our dataset is balanced or not by checking our target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "22-VNtiv7Rf_",
    "outputId": "386d7918-7075-48a4-c196-110c4ed2ad6f"
   },
   "outputs": [],
   "source": [
    "df.bank_account.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-8fQPREL7RgB",
    "outputId": "85838537-2c3a-4717-b94a-8efcb155a4e4"
   },
   "outputs": [],
   "source": [
    "a = len(df[df.bank_account=='Yes'])\n",
    "b = len(df[df.bank_account=='No'])\n",
    "c = len(df)\n",
    "print('We have an imbalanced dataset with a %i/%i ratio'%((b/c*100),(a/c*100)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xGf8YbNd7RgD"
   },
   "source": [
    "Stratification will be needed when doing cross-validation to preserve this ratio in our folds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60lne_QK7RgE"
   },
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDLc4d7d7RgG"
   },
   "source": [
    "### Age of respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "wYYI8oSZ7RgG",
    "outputId": "660b34ad-f163-4e4f-a555-e1385e35c9f6"
   },
   "outputs": [],
   "source": [
    "hist_age = df.age_of_respondent.hist(bins=25,figsize=[15,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kxvL9xu7RgJ"
   },
   "source": [
    "### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "XrCEAdmC7RgK",
    "outputId": "1d2cc481-93ac-4750-d6f9-844b336328f7"
   },
   "outputs": [],
   "source": [
    "df['country'].value_counts().plot(kind='bar',figsize=[15,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKZbD4Kj7RgM"
   },
   "source": [
    "### Let's check the year variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "z7SbP-Ga7RgN",
    "outputId": "dc63d7ba-33ec-4022-830f-7e9de6089d71"
   },
   "outputs": [],
   "source": [
    "df.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3RbdQMfW7RgP",
    "outputId": "b8915657-7bdd-43a5-ff43-d291309cefb5"
   },
   "outputs": [],
   "source": [
    "df[df.year==2016].country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qFWcnsX07RgR",
    "outputId": "7ad3a16b-8a4a-4ea0-f1d0-ee43c07062e3"
   },
   "outputs": [],
   "source": [
    "df[df.year==2017].country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "-SN9-GNw7RgU",
    "outputId": "9fbed74e-cc81-4e66-fe76-272119ec9efd"
   },
   "outputs": [],
   "source": [
    "df[df.year==2018].country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7HYNVDh7RgW"
   },
   "source": [
    "### Bivariate Analysis using the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNdWApHU7RgX"
   },
   "outputs": [],
   "source": [
    "## target encoding\n",
    "di = {\"Yes\": 1, \"No\": 0}\n",
    "df.replace({\"bank_account\": di},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B84WxA2_7RgZ"
   },
   "source": [
    "### Age of Respondent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "colab_type": "code",
    "id": "2cQkGaiw7RgZ",
    "outputId": "9739cdb0-3796-4645-bc37-ca7cefa17f1e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[18,12])\n",
    "sns.barplot('age_of_respondent', 'bank_account', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sOBIFuAn7Rgc"
   },
   "source": [
    "We can safely say that ,generally speaking and by also looking at the general trend of the plot, older people are less likely to have a bank_account. We have some outliers beyond the 90 mark for the age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "syIgiWL47Rgc"
   },
   "source": [
    "### Gender of Respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "vIgYELVs7Rgd",
    "outputId": "a8db4f57-233f-4187-f7e5-8305a2831f2c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,6])\n",
    "sns.barplot('gender_of_respondent', 'bank_account', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwtvfMVX7Rgh"
   },
   "source": [
    "Males are more likely to have a bank account according to this plot. Maybe we can combine gender and age, and see if put together, we could notice something and create a feature that better represents this combination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1UZea087Rgj"
   },
   "source": [
    "### Age + Gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "Kc9Rc7E37Rgk",
    "outputId": "1861f6cb-7a0a-42e7-f662-0e4aef74c262"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action=\"ignore\")\n",
    "plt.figure(figsize=[25,20])\n",
    "plt.subplot(331)\n",
    "sns.distplot(df[(df.gender_of_respondent=='Male')&(df.bank_account==1)]['age_of_respondent'].dropna().values, bins=range(0, 100, 1), kde=False, color='red')\n",
    "sns.distplot(df[(df.gender_of_respondent=='Male')&(df.bank_account==0)]['age_of_respondent'].dropna().values, bins=range(0, 100, 1), kde=False, color='blue',\n",
    "            axlabel='Males age')\n",
    "plt.subplot(332)\n",
    "sns.distplot(df[(df.gender_of_respondent=='Female')&(df.bank_account==1)]['age_of_respondent'].dropna().values, bins=range(0, 100, 1), kde=False, color='red')\n",
    "sns.distplot(df[(df.gender_of_respondent=='Female')&(df.bank_account==0)]['age_of_respondent'].dropna().values, bins=range(0, 100, 1), kde=False, color='blue',\n",
    "            axlabel='Females age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "AdnP8IMs7Rgp",
    "outputId": "a83f3bcd-dd0e-4258-ec81-4bd30aadaf8f"
   },
   "outputs": [],
   "source": [
    "df.gender_of_respondent.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfhBA8iq7Rgr"
   },
   "source": [
    "For both genders, the peak in the likelihood of having a bank_account happens between 20-40. But the number of males , keeping in mind that in the trainset they are less than females by 4k, who have a bank_account is closer to those who don't compared to females. Meaning, age plays a role for both genders almost the same way, but gender has an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGatUiAd7Rgs"
   },
   "source": [
    "### Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "6n5wNwWw7Rgu",
    "outputId": "785430e2-a607-4d18-98b7-bac5abc60473"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,6])\n",
    "sns.barplot('country', 'bank_account', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gi9x0vcM7Rgv"
   },
   "source": [
    "### Household Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "JeXvVZTw7Rgw",
    "outputId": "5457ef18-fe4c-4dbc-ca22-1a3731f9221f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,6])\n",
    "sns.barplot('household_size', 'bank_account', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6HixC6F7Rgy"
   },
   "source": [
    "Data needs to be cleaned to get something useful out of this variable. as you can see the bigger values has fewer samples and they might also have outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3KrY13q7Rgz"
   },
   "source": [
    "### Marital Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "colab_type": "code",
    "id": "ZgcWWsXu7Rg0",
    "outputId": "629f28f9-bb25-451e-e9be-a43f9e721da4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,6])\n",
    "sns.barplot('marital_status', 'bank_account', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNcklZ0j7Rg2"
   },
   "source": [
    "almost similar across all categories, the undersampled Dont know with 38 samples might cause a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkMY1DrM7Rg2"
   },
   "source": [
    "### Lets see interactions between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "02jfmGiT7Rg3",
    "outputId": "c2c0ecb6-44ea-4dd7-b5a5-9d531c6f5fee"
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"household_size\", y=\"bank_account\", hue=\"gender_of_respondent\", col=\"country\",\n",
    "                   data=df, aspect=0.9, size=4, ci=95.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svCSKeOP7Rg5"
   },
   "source": [
    "for Kenyans, we can see that household_size is inversely proportionnal to the target, except for some outliers that mess up the plot at bigger household_size values. For Tanzanians , the decline is clearer with lesser outliers. Uganda and Rwanda also have a small decline but the outliers are at it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "s4fDWsgi7Rg6",
    "outputId": "1c9c90e9-79c3-4a66-b0e9-4f27603c1076"
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"household_size\", y=\"bank_account\", hue=\"gender_of_respondent\", col=\"location_type\",\n",
    "                   data=df, aspect=0.9, size=6, ci=95.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmBpe-Ab7Rg9"
   },
   "source": [
    "### Binnedage-based and gender-based plots vs target and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MvOuPRaN7Rg9"
   },
   "outputs": [],
   "source": [
    "## binning the age!\n",
    "bins = [10, 20, 30, 40,60,80,100]\n",
    "labels = [1,2,3,4,5,6]\n",
    "df['binnedage'] = pd.cut(df['age_of_respondent'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "wn61DKmv7RhA",
    "outputId": "c4fa498c-5665-42f8-b286-92fa102f6bdd"
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"binnedage\", y=\"bank_account\", hue=\"gender_of_respondent\",\n",
    "                   data=df, aspect=0.9, size=5, ci=95.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdc4zOsI7RhE"
   },
   "source": [
    "Both genders follow the same pattern, it gets the highest between 20-30, and then starts slowly decreasing. No need to create a new feature based on feature interaction age-gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "1_O5XFaN7RhF",
    "outputId": "b55a302d-d577-4f27-8a8a-488800bb34ca"
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"binnedage\", y=\"bank_account\", hue=\"education_level\",col='gender_of_respondent',\n",
    "                   data=df, aspect=0.9, size=5, ci=95.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7GxaiKF7RhI"
   },
   "source": [
    "Males and females share the same pattern for educaton level for the same ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "mnqqvg0m7RhI",
    "outputId": "31b43382-0f30-4f40-c94f-c31572568f31"
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(x=\"binnedage\", y=\"bank_account\", hue=\"job_type\",col='gender_of_respondent',\n",
    "                   data=df, aspect=0.9, size=8, ci=95.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zu3y0DDd7RhK"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing \n",
    "# convert categorical features to numerical features\n",
    "# categorical features to be converted by One Hot Encoding\n",
    "le = LabelEncoder()\n",
    "df['country_'] = df['country']\n",
    "\n",
    "categ = ['relationship_with_head', 'marital_status', 'education_level', 'job_type', 'country_']\n",
    "# One Hot Encoding conversion\n",
    "df = pd.get_dummies(df, prefix_sep='_', columns = categ)\n",
    "\n",
    "# Labelncoder conversion\n",
    "df['location_type'] = le.fit_transform(df['location_type'])\n",
    "df['cellphone_access'] = le.fit_transform(df['cellphone_access'])\n",
    "df['gender_of_respondent'] = le.fit_transform(df['gender_of_respondent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssW1adIf7RhO"
   },
   "source": [
    "## Module 2: Decision Trees and Random Forests\n",
    "\n",
    "In this module, you will implement two popular and extremely powerful Machine Learning models - Decision Trees and Random Forests - using Python and scikit-learn. For every classification model built with scikit-learn, we will follow four main steps: 1) **Building or instantiating ** the classification model (using either default, pre-defined or optimised parameters), 2) **Training** the model, 3) **Testing** the model, and 4) **Performance evaluation** using various metrics to test its generalisation ability.  Thorough validation techniques will be applied throughout these steps as a means of ensuring real-world metrics and avoiding cases of overfitting (or underfitting). Finally, you will learn how to optimise the hyperparameters of a model as a way of boosting its overall performance. \n",
    "\n",
    "\n",
    "### Learning Activity: Split the data into training and test sets\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split the online retail dataset into two disjoint sets: train and test (**Holdout method**) using the `train_test_split()` function. \n",
    "\n",
    "The `random_state` argument specifies a value for the seed of the random generator. By setting this seed to a particular value, each time the code is executed, the split between train and test datasets will be exactly the same. If this value is not specified, a different split will be performed each time since the random generator driving the split will be seeded by a pseudo-random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eb9U_a1L7RhP"
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X = df.drop([\"uniqueid\", \"bank_account\",\"year\",\"country\"], axis = 1)\n",
    "Y = df[\"bank_account\"]\n",
    "X_train, X_test, y_train,y_test=train_test_split(X,Y, test_size=0.3, random_state=47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xnQWY0q7RhR"
   },
   "source": [
    "The output of `train_test_split()` consists of four arrays. _XTrain_ and _yTrain_ are the two arrays you use to train your model. _XTest_ and _yTest_ are the two arrays that you use to evaluate your model. By default, scikit-learn splits the data so that 25% of it is used for testing, but you can also specify the proportion of data you want to use for training and testing. You can check http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html on how to set this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "C09MwL-k7RhS",
    "outputId": "72ec2fe9-1efa-4f95-ea1d-af3111feb89e"
   },
   "outputs": [],
   "source": [
    "# Add code here to explore Xtrain, ytrain etc.. \n",
    "#(eg: frequency of Y, head of X, dimensionality, ...)\n",
    "# Print the dimensionality of the individual splits\n",
    "\n",
    "print(\"XTrain dimensions: \", X_train.shape)\n",
    "print(\"yTrain dimensions: \", y_train.shape)\n",
    "print(\"XTest dimensions: \",  X_test.shape)\n",
    "print(\"yTest dimensions: \",  y_test.shape)\n",
    "\n",
    "\n",
    "yFreq = scipy.stats.itemfreq(y_test)\n",
    "print(yFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tt-X68ee7RhU"
   },
   "source": [
    "If you look at the frequency of `yTest`, you will see that 59 random samples of class 0 (non-returning customers) and 441 random samples of class 1 (returning customers) are included in the yTest set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXge3s7F7RhU"
   },
   "source": [
    "### Learning Activity:  Decision Trees\n",
    "\n",
    "Decision Tree classifiers construct classification models in the form of a tree structure. A decision tree progressively splits the training set into smaller subsets. Each node of the tree represents a subset of the data. Once a new sample is presented to the data, it is classified according to the test condition generated for each node of the tree.\n",
    "\n",
    "Let us build a simple decision tree with 3 layers. (See [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for the documentation of the Decision Tree classifier.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFI40A3r7RhV"
   },
   "outputs": [],
   "source": [
    "# Building the classification model using a pre-defined parameter\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=3) \n",
    "# Train the model\n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "# Test the model\n",
    "yPred = dtc.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59J5_gi_7RhX"
   },
   "source": [
    "### Learning Activity: Calculate validation metrics for your classifier\n",
    "\n",
    "In a classification task, once you have created your predictive model, you will need to evaluate it. Evaluation functions help you to do this by reporting the performance of the model through four main performance metrics: precision, recall and specificity for the different classes, and overall accuracy. To understand these metrics, it is useful to create a _confusion matrix_, which records all the true positive, true negative, false positive and false negative values.\n",
    "\n",
    "We can compute the confusion matrix for our classifier using the `confusion_matrix` function in the `metrics` module. The inputs are the `yTest` and `yPred`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "E7GED6lo7RhX",
    "outputId": "31a52271-9b52-4cb3-f5bf-c9859bc1902c"
   },
   "outputs": [],
   "source": [
    "\n",
    "mat = metrics.confusion_matrix(y_test, yPred) \n",
    "print (mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lBNPdic7RhZ"
   },
   "source": [
    "\n",
    "Because performance metrics are such an important step of model evaluation, scikit-learn offers a wrapper around these functions, `metrics.classification_report`, to facilitate their computation. It also offers the function `metrics.accuracy_score` that we tried before to compute the overall accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "76ovoCK87Rha",
    "outputId": "c22fdf94-cc45-4c9e-d98d-4ec72ecc42de"
   },
   "outputs": [],
   "source": [
    "# Report the metrics using metrics.classification_report\n",
    "print (metrics.classification_report(y_test, yPred))\n",
    "print (\"Overall Accuracy: \", round(metrics.accuracy_score(y_test, yPred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2cfE9HU7Rhd"
   },
   "source": [
    "### Learning Activity:  Random Forests\n",
    "\n",
    "The random forests model is an _ensemble method_ since it aggregates a group of decision trees into an [ensemble](http://scikit-learn.org/stable/modules/ensemble.html). Ensemble learning involves the combination of several models to solve a single prediction problem. It works by generating multiple classifiers/models which learn and make predictions independently. Those predictions are then combined into a single (mega) prediction that should be as good or better than the prediction made by any one classifer. Unlike single decision trees which are likely to suffer from high variance or high bias (depending on how they are tuned) Random Forests use averaging to find a natural balance between the two extremes. <br/> \n",
    "\n",
    "Let us start by building a simple Random Forest model which consists of 150 independently trained decision trees. For further details and examples on how to construct a Random Forest, see [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "I20_CDz67Rhg",
    "outputId": "faf7a61a-3b4e-4da2-9641-1b6d16e55a6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build a Random Forest classifier with 150 decision trees\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "predRF = rf.predict(X_test)\n",
    "\n",
    "mat = metrics.confusion_matrix(y_test, predRF) \n",
    "print (mat)\n",
    "\n",
    "print(metrics.classification_report(y_test, predRF))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, predRF),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vghi7wzr7Rhi"
   },
   "source": [
    "### Learning Activity: Feature Importance \n",
    "\n",
    "Random forests allow you to compute a heuristic for determining how “important” a feature is in predicting a target. This heuristic measures the change in prediction accuracy if you take a given feature and permute (scramble) it across the datapoints in the training set. The more the accuracy drops when the feature is permuted, the more “important” we can conclude the feature is.\n",
    "\n",
    "We can use the `feature_importances_` attribute of the RF classifier to obtain the relative importance of each feature, which we can then visualise using a simple bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5p-PYA5Q7Rhj",
    "outputId": "09680267-7409-493f-ed6d-30f1761ec43f"
   },
   "outputs": [],
   "source": [
    "# Display the importance of the features in a barplot\n",
    "\n",
    "# sorting the features according their importance\n",
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "_LgqjNNX7Rhl",
    "outputId": "91c09d5b-97d4-4edd-b2bf-a61d7bdefb05"
   },
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(4).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqUNw5Tr7Rho"
   },
   "source": [
    "### Learning Activity: Tuning Random Forests with grid search\n",
    "\n",
    "Random forests offer several parameters that can be tuned. In this case, parameters such as `n_estimators`, `max_features`, `max_depth` and `min_samples_leaf` can be some of the parameters to be optimised. The optimal choice for these parameters is highly *data-dependent*. Rather than trying one-by-one predefined values for each hyperparameter, we can automate this process. The scikit-learn library provides the grid search function [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html), which allows us to exhaustively search for the optimum combination of parameters by evaluating models trained with a particular algorithm with all provided parameter combinations. Further details and examples on grid search with scikit-learn can be found [here](http://scikit-learn.org/stable/modules/grid_search.html). You can use the `GridSearchCV` function with the validation technique of your choice (in this example, 10-fold cross-validation has been applied) to search for a parametisation of the RF algorithm that gives a more optimal model.\n",
    "\n",
    "As a first step, create a dictionary of allowed parameter ranges for `n_estimators` and `max_depth` (or include more of the parameters you would like to tune) and conduct a grid search with cross validation using the `GridSearchCV` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dKdzjFwt7Rhp",
    "outputId": "36974224-4dbe-473d-ae28-376b904cbbdc"
   },
   "outputs": [],
   "source": [
    "# Conduct a grid search with 5-fold cross-validation using the dictionary of parameters\n",
    "# Parameters you can investigate include:\n",
    "n_estimators = np.arange(5, 100, 25)\n",
    "max_depth    = np.arange(1, 35, 5)\n",
    "# percentage of features to consider at each split\n",
    "max_features = np.linspace(.1, 1.,3)\n",
    "parameters   = [{'n_estimators': n_estimators,\n",
    "                 'max_depth': max_depth,\n",
    "                 'max_features': max_features}]\n",
    "\n",
    "gridCV = GridSearchCV(RandomForestClassifier(), param_grid=parameters, cv=5, n_jobs=4) \n",
    "gridCV.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters\n",
    "best_n_estim      = gridCV.best_params_['n_estimators']\n",
    "best_max_depth    = gridCV.best_params_['max_depth']\n",
    "best_max_features = gridCV.best_params_['max_features']\n",
    "\n",
    "print ('Best parameters: n_estimators=', best_n_estim,\n",
    "       'max_depth=', best_max_depth,\n",
    "       'max_features=', best_max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvghs_nm7Rhs"
   },
   "source": [
    "By default, parameter search uses overall accuracy (`sklearn.metrics.accuracy_score`) as a metric in classification. For some applications, other scoring functions and metrics are better suited (for example in _unbalanced classification_, the overall accuracy score may often be misleading). An alternative scoring function such as the ones provided at http://scikit-learn.org/stable/modules/model_evaluation.html can be specified via the `scoring` parameter in `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkOthtut7Rht"
   },
   "source": [
    "### Learning Activity: Testing and evaluating the generalisation performance\n",
    "\n",
    "When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process (_XTest_). So, we are testing our independent _XTest_ dataset using the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b7EXkpQ7Rht"
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the *optimal* parameters detected by grid search\n",
    "clfRDF = RandomForestClassifier(n_estimators=best_n_estim,\n",
    "                                max_depth=best_max_depth,\n",
    "                                max_features = best_max_features)\n",
    "\n",
    "clfRDF.fit(X_train, y_train)\n",
    "predRF = clfRDF.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OVzVtQI7Rhw"
   },
   "source": [
    "Use the `classification_report` from the `metrics` module and the `accuracy_score` to see how well the classifier is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "TxR3Yz2M7Rhx",
    "outputId": "c8f788bd-1e3c-4ddb-9593-d8367e3bfcda"
   },
   "outputs": [],
   "source": [
    "# add your code here\n",
    "print (metrics.classification_report(y_test, predRF))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, predRF),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYm5OAnr7Rhy"
   },
   "source": [
    "## Module 3: Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) attempt to build a decision boundary that accurately separates the samples of different classes by *maximising* the margin between them.\n",
    "\n",
    "### Learning Activity: Linear SVMs\n",
    "\n",
    "At first, let us build a linear SVM model using the _default_ value for the hypeparameter `C` (based on the scikit-learn documentation, the default case is `C = 1.0`). The regularisation `C` trades off misclassification of training examples against simplicity of the decision surface. A low `C` tolerates training misclassifications and allows softer margins, while for high `C` the misclassifications become more significant leading to hard-margin SVMs and potentially cases of overfitting.\n",
    "\n",
    "Thorough documentation on how to implement linear SVMs with scikit-learn can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "jcmiPuOM7Rhz",
    "outputId": "494a29e9-8879-4ee9-dea4-041caf4f6c5b"
   },
   "outputs": [],
   "source": [
    "# Build a linear SVM classifier with the default hyperparameter C\n",
    "# (where C = 1.0; this argument is optional and could be omitted)\n",
    "linearSVM = SVC(kernel='linear', C=1.0)\n",
    "linearSVM.fit(X_train, y_train)\n",
    "yPred = linearSVM.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, yPred))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, yPred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLREHRtr7Rh1"
   },
   "source": [
    "### Learning Activity: Non-linear (RBF) SVMs\n",
    "\n",
    "In addition to the regularisation parameter `C`, which is common for all types of SVM, the gamma hyperparameter in the RBF kernel controls the nonlinearity of the SVM bounaries. The larger the gamma, the more nonlinear the boundaries surrounding individual samples. Lower values of gamma lead to broader, more linear boundaries. <br/>  \n",
    "\n",
    "At first, let us build an RBF SVM model (set the `kernel` parameter to `rbf`) using the default values for the hypeparameters `C` (`C=1.0`) and `gamma` (`gamma='auto'`). Thorough documentation on how to implement SVMs with scikit-learn can be found at http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "1tgfbWgW7Rh2",
    "outputId": "9bbf4ac4-7aed-4953-e53a-f70a4052a999"
   },
   "outputs": [],
   "source": [
    "# Build a non-linear (RBF) classifier using the default parameters for C and gamma\n",
    "\n",
    "rbfSVM = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "rbfSVM.fit(X_train, y_train)\n",
    "yPred = rbfSVM.predict(X_test)\n",
    "\n",
    "print (metrics.classification_report(y_test, yPred))\n",
    "print (\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, yPred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABgeikK57Rh7"
   },
   "source": [
    "## END OF DAY"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Copy of Introduction to Machine Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
