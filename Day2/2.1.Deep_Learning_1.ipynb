{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD3y6d9YDxiL"
   },
   "source": [
    "# Practical 3: Deep Feed-forward Networks\n",
    "**IndabaX sudan 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gUyCHHYDxiO"
   },
   "source": [
    "## Introduction\n",
    "In this practical, we implement and train a feed-forward neural network (also known as an \"MLP\" for \"multi-layer perceptron\") on a dataset called **\"Fashion MNIST\"**, consisting of small greyscale images of items of fashion. We consider the practical issues around generalisation to out-of-sample data and introduce some important techniques for addressing this.\n",
    "\n",
    "Before diving into the Neural network part we first have an introduction to Tensorflow, a very popular deep learning framework that is used in all aspects of defining, training and testing models. \n",
    "## Learning objectives\n",
    "* Understand the basics of Tensorflow\n",
    "* Use **Tensorflow Eager** and **Keras Layers** to build a neural network archetecture\n",
    "* Understand how a model is trained and evaluated\n",
    "* Differentiate between **in-sample** and **out-of-sample** model performance\n",
    "* Understand the concept of **train/validation/test split** and why it's useful\n",
    "* Research at least 1 techniques that can be used to improve model **generalisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PhRp2b3DxiP"
   },
   "source": [
    "# Tensorflow\n",
    "As described in Wikipedia TensorFlow is a computational framework for building machine learning models. It is the second generation system from Google Brain headed by Jeff Dean. Launched in early 2017, it has disrupted the ML world by bringing in numerous capabilities from scalability to building production ready models.\n",
    "\n",
    "TensorFlow (TF) is an open source software library for numerical computation using the concept of Tensors. You can think of Tensors as being a generalisation of matrices to higher dimensions, or roughly equivalent to multi-dimensional arrays. Scalars are 0-dimensional tensors, vectors are 1-dimensional, standard matrices are 2-dimensional, and higher-dimensional tensors have 3 or more dimensions. You can think of dimensions as representing groups of numbers that mean the same thing. For example, for images, we often use 3-dimensional tensors where the first dimension represents the red, green, and blue color channels of the image, and the next two are the columns and rows of pixels of the image. \n",
    "\n",
    "**Note**: Don't be confused when people say \"2-D vector\" or \"3-D vector\", which refers to a 1-dimensional tensor that has size 2 or 3.\n",
    "\n",
    "The major advantage of using TensorFlow is that it can automatically derive the gradients of many mathematical expressions involving tensors. It achieves this through a process called \"automatic differentiation\". Tensorflow also supports multiple \"kernels\", allowing you to easily run your code on normal processors (CPUs), graphics cards (GPUs) and other more exotic hardware accelerators like Google's Tensor Processing Units (TPUs)\n",
    "\n",
    "Tensorflow actually provides **two modes of operation**, the first, called \"graph mode\", builds a computation graph upfront and then feeds data into the graph. By building the graph upfront, Tensorflow can apply optimisations to the graph that allow it to extract peak performance from the hardware you're running on. You will have encountered this mode if you used Tensorflow before or attended the Indaba last year! The second mode, called [\"Eager-mode\"](https://www.tensorflow.org/guide/eager), is a lot newer and evaluates Tensor operations imperatively (in the order you write them), similar to NumPy and PyTorch. Eager-mode is slightly less performant but a lot more intuitive, especially if you've never used a \"define-and-run\" programming style (like graph mode) before, and is therefore the mode we will use in these practicals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b32FKSRGDxiQ"
   },
   "outputs": [],
   "source": [
    "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
    "\n",
    "#!pip install tensorflow-gpu==2.0.0-beta0 > /dev/null 2>&1\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#print(\"TensorFlow executing eagerly: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm_iqNqSDxiW"
   },
   "source": [
    "For statrers, we can define a constant which is a tensor whose value cannot be changed at all. It can take many shapes and dimentions as the examples below show.\n",
    "\n",
    "We can also declare variable tensors which values change. Their declaration is abit different but the code below shows the difference between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBHp96yGDxiX",
    "outputId": "b77bead5-ed2c-49c3-da2e-ab854bafa410"
   },
   "outputs": [],
   "source": [
    "#for declaring constants we use the ft.constant function\n",
    "x1 = tf.constant(3)            #shape ()\n",
    "x2 = tf.constant([3, 5, 7])    #shape (3,)\n",
    "#we can use tf.stack to create higher dimentinal matricies\n",
    "x3 = tf.stack([x2, x2])        #shape (2, 3)\n",
    "\n",
    "#for declaring a variable we use the function tf.get_variable and we manuplate them using the tf.variable class\n",
    "\n",
    "\n",
    "my_var = tf.compat.v1.get_variable(\"my_var\", [1, 2, 3])\n",
    "\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "print (x1)\n",
    "print(x2)\n",
    "print(x3) \n",
    "print(my_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNUhqopdDxic"
   },
   "source": [
    "As you might have noticed, running the above code results in printing tensor abstract but not the values themselves. This is because a Tensorflow code is just an abstract representation of what will be run in the Tensorflow session. A session encapsulates state of Tensorflow runtime and runs Tensorflow operations. Tesnorflow represents the written code in an unserlying graph. Take the following code, it is represented by the grave underneath it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAItjczzDxid"
   },
   "outputs": [],
   "source": [
    "# sessions = tf.compat.v1.Session()\n",
    "x = tf.constant([1, 2, 3], name = \"x\")\n",
    "y = tf.constant([4, 5, 6], name = \"y\")\n",
    "z1 = tf.add(x, y, name = \"z1\")\n",
    "z2 = x * y\n",
    "z3 = z2- z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj-olbnzDxii"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/314/1*B9L6Y6lz1NzFFjyC1YMeYA.png)\n",
    "\n",
    "when we run the session thats when the value are inputed and we can get the values of the Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "in42xVAiDxij",
    "outputId": "79a3da3b-f4af-438f-8c66-e75f64f62d82"
   },
   "outputs": [],
   "source": [
    "# print(sessions.run(z3))\n",
    "# sessions.close()\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9BV8aKGDxio"
   },
   "source": [
    "### Basic TensorFlow operations\n",
    "#### Addition and Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNINon7sDxip",
    "outputId": "364d77ad-62ba-4262-d9f1-acf845ec6d52"
   },
   "outputs": [],
   "source": [
    "a1 = tf.constant([1, 2, 3])\n",
    "a2 = tf.constant([3, 4, 5])\n",
    "a3 = a1 + a2 # (tf.add(a1, a2) also works. TensorFlow supports primitive operators)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     print(session.run(a3))\n",
    "print(a3)\n",
    "\n",
    "# Result [4, 6, 8]\n",
    "\n",
    "a4 = a1 - a2\n",
    "# with tf.Session() as session:\n",
    "#     print(session.run(a4))\n",
    "print(a4)\n",
    "\n",
    "# Result [-2, -2, -2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FyGpbYdDxir"
   },
   "source": [
    "#### Multiplication and Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufabru9SDxis",
    "outputId": "cb3c1bc7-36cf-409b-8ad0-6a160c1672a2"
   },
   "outputs": [],
   "source": [
    "a1 = tf.constant([1, 2, 3])\n",
    "a2 = tf.constant([3, 4, 5])\n",
    "a3 = a1 * a2 # (tf.mul(a1, a2) also works. TensorFlow supports primitive operators)\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     print(session.run(a3))\n",
    "print(a3)\n",
    "\n",
    "# Result [3, 8, 15]\n",
    "\n",
    "a4 = a1/a2\n",
    "# with tf.Session() as session:\n",
    "#     print(a4.eval())  # eval() can be used instead of session.run() to compute the results of a particular variable.\n",
    "\n",
    "print(a4)\n",
    "\n",
    "# Result [0.34, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QWxcFumDxiv"
   },
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMHsPNZgDxiv",
    "outputId": "db754752-c3dc-4f79-958b-bcbd1d1e3423"
   },
   "outputs": [],
   "source": [
    "a1 = tf.constant(([3, 5, 7],\n",
    "                 [4, 6, 8]))  # Shape is (2, 3)\n",
    "a2 = tf.reshape(a1, [3, 2])\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(a2.eval())\n",
    "\n",
    "print(a2)\n",
    "    \n",
    "# Result[[3, 5], [7, 4], [6, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxQTVKDUsV33"
   },
   "source": [
    "## Now restart the runtime and run the cell below to install Tensorflow v2.0 and start using Tensorflow Eager mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHKaR1Mda3-D",
    "outputId": "43abb713-1032-4353-b1cf-91990be85d17"
   },
   "outputs": [],
   "source": [
    "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
    "\n",
    "# !pip install tensorflow-gpu==2.0.0-beta0 > /dev/null 2>&1\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "print(\"TensorFlow executing eagerly: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB9yL17iDxiy"
   },
   "source": [
    "Now that we got the basics of tensorflow we can delve into the meat of the practical and start on the Feed-forward Network\n",
    "### The Data\n",
    "In this practical, we use the Fashion MNIST dataset consisting of 70,000 greyscale images and their labels. The dataset is divided\n",
    " into 60,000 training images and 10,000 test images. The idea is to train a **classifier** to identify the class value (what type of fashion item it is) given the image. We train and *tune* a model on the 60,000 training images and then evaluate how well it classifies the 10,000 test images that the model did not see during training. This task is an example of a **supervised learning** problem, where we are given both input and labels (targets) to learn from. This is in contrast to **unsupervised learning** where we only have inputs from which to learn patterns or **reinforcement learning** where an agent learns how to maximise a reward signal through interaction with its environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJbiiRT_Dxiz"
   },
   "outputs": [],
   "source": [
    "# Tensorflow has convenient modules for loading a number of standard datasets\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_and_validation_images, train_and_validation_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "text_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPUdyKBpDxi1"
   },
   "source": [
    "### Optional extra reading: Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUd4N-7dDxi3"
   },
   "source": [
    "When we build machine learning models, the goal is to build a model that will perform well on *future* data that we have not seen yet. We say that we want our models to be able to **generalise** well from whatever training data we can collect and do have available, to whatever data we will be applying them to in future. To do this, we split whatever data we have available into a **training set, a validation set and a test set**. The idea is that we train our model and use the performance on the validation set to make any adjustments to the model and its hyperparameters, but then we report the final accuracy on the test set. The test set (which we never train on), therefore acts as a proxy for our future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFLhPVy1Dxi3"
   },
   "source": [
    "### Construct a validation set\n",
    "Next, we remove 10,000 images and labels from the training set to use as a validation set. We will come back to the validation set later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZGBn7mIDxi4"
   },
   "outputs": [],
   "source": [
    "# Construct a validation set from the last 10000 images and labels from\n",
    "# train_and_validation_images and train_and_validation_labels\n",
    "validation_images = train_and_validation_images[-10000:, :, :]\n",
    "validation_labels = train_and_validation_labels[-10000:]\n",
    "\n",
    "# Construct a training set from the first 50000 images and labels.\n",
    "train_images = train_and_validation_images[:50000, :, :]\n",
    "train_labels = train_and_validation_labels[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbblWjSnDxi6"
   },
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 28 x 28 matrix of greyscale pixels. The values are between 0 and 255 where 0 represents black, 255 represents white and there are many shades of grey in-between. Each image is assigned a corresponding numerical label, so the image in ```train_images[i]``` has its corresponding label stored in ```train_labels[i]```. We also have a lookup array called ```text_labels``` to associate a text description with each of the numerical labels. For example, the label 1 is associated with the text description \"Trouser\".\n",
    "\n",
    "The 2 cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "OcQBBEcEDxi7",
    "outputId": "c4414330-d7be-42e1-b768-298dc698b0e7"
   },
   "outputs": [],
   "source": [
    "# We use the Matplotlib plotting library to visualise an image selected at random from the training set\n",
    "plt.figure()\n",
    "random_index = np.random.randint(0, len(train_images))\n",
    "plt.imshow(train_images[random_index], cmap='gray_r')\n",
    "plt.colorbar()\n",
    "numerical_label = train_labels[random_index]\n",
    "text_description = text_labels[numerical_label]\n",
    "plt.title('True Class: {} (\"{}\")'.format(numerical_label, text_description))\n",
    "\n",
    "plt.gca().grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "3LpNPO5PDxi9",
    "outputId": "791b1ee1-c5fd-41bf-e399-19c64fe7bf6c"
   },
   "outputs": [],
   "source": [
    "# Another view, showing 25 randomly selected images at a time\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = np.random.randint(0, 50000)\n",
    "    plt.imshow(train_images[img_index], cmap=\"gray_r\")\n",
    "    plt.xlabel(text_labels[train_labels[img_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCRfDi_lDxjA"
   },
   "source": [
    "### Exploratory Task\n",
    "Run the above cells multiple times to get a good sense of the dataset. Look at the general structure of the images and their corresponding labels. Can you spot some classes that might be difficult for a classifier to distinguish? If so, why do you think they would be difficult to distinguish? Chat to your neighbour about this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wey6RimYDxjB"
   },
   "source": [
    "### Preparing the data with TensorFlow\n",
    "At the moment, our training data consists of two large tensors. The images are stored in a tensor of shape $[50000, 28, 28]$, consisting of all the $28 \\times 28$ images matrices stacked together. The labels are stored in a 1D vector of shape $[50000]$. We wish to train a model using **mini-batch stochastic gradient descent**. In order to do so, we need to shuffle the data and split it into smaller (mini-)batches. We also convert the data from numpy arrays to TensorFlow Tensors.\n",
    "\n",
    "\n",
    "In order to do this batching (and shuffling) we will use the Tensorflow [Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), which is a set of simple reusable components that allow you to build efficient data pipelines. Data is said to \"stream\" through the pipeline, meaning that when something at the output of the pipeline wants data, the pipeline will provide that data as soon as it has enough, rather than waiting to process all the data. This allows you to easily build pipelines that work on large datasets without having to load it all into memory in one go!\n",
    "\n",
    "We build this pipeline step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Cav6zIFDxjC"
   },
   "source": [
    "We start by defining the ```batch_size``` hyperparameter of our model. This hyperparameter controls the sizes of the mini-batches (chunks) of data that we use to train the model. The value you use will affect the memory usage, speed of convergence and potentially also the performance of the model. It also interacts with the *learning rate* used in gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPtwHJtXDxjD"
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99JDcc0hDxjG"
   },
   "source": [
    "The first component we add will group the image and label tensors together into a tuple and then split them into individual entries - ie. 50000 tuples containing a (28, 28) dimensional image and 1D label associated with that image. The following line adds this splitting component to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBQucqtvDxjH"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoDJgveUDxjL"
   },
   "source": [
    "The next thing we do is apply a map operation. This lets us run an arbitrary function on each element. The function we provide returns the image values divided by 255 and converted ('cast') to a float and the label converted to a 32-bit integer.\n",
    "\n",
    "**NOTE**: \"Lambda\" functions are just one-line, anonymous functions. In this case, it defines a function that takes arguments x and y (before the colon), and outputs their manipulated values (after the colon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBhiIxk7DxjN"
   },
   "outputs": [],
   "source": [
    "# Divide image values and cast to float so that they end up as a floating point number between 0 and 1\n",
    "train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv2sIJfjDxjS"
   },
   "source": [
    "Then we add a **shuffle** component. This returns a random element from the pipeline. Can you spot a potential problem here? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBRwScQSDxjU"
   },
   "outputs": [],
   "source": [
    "# Shuffle the examples.\n",
    "train_ds = train_ds.shuffle(buffer_size=batch_size * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgVg1yitDxje"
   },
   "source": [
    "The final component in our pipeline is the batch component. This just requests `batch_size` elements from the previous pipeline component, groups them together into a single tensor and returns that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtwhLQGODxji"
   },
   "outputs": [],
   "source": [
    "# Now \"chunk\" the examples into batches\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "# The output of this pipeline will be tuples of tensors containing images and labels.\n",
    "# The images will be of shape (batch_size, 28, 28) and the labels of shape (batch_size, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF8OIlSyDxjn"
   },
   "source": [
    "Finally, we apply the same pre-processing (converting to Tensors and normalising the values to lie between 0 and 1) to the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKeX5wkkDxjo"
   },
   "outputs": [],
   "source": [
    "# Don't worry about this for now, we will use the validation set later\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
    "val_ds = val_ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, tf.cast(y, tf.int32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C4s7E_XDxjq"
   },
   "source": [
    "## Define the model\n",
    "In this section we'll build a classifier. A **classifier** is a function that takes an object's characteristics (or \"features\") as inputs and outputs a prediction of the class (or group) that the object belongs to. It may make a single prediction for each input or it may output some score (for example a probability) for each of the possible classes. Specifically, we will build a classifier that takes in (a batch of) 28 x 28 Fashion MNIST images as we've seen above, and outputs predictions about which class the image belongs to. \n",
    "\n",
    "For each (batch of) input images, we will use a **feed-forward neural network** to compute un-normalised scores (also known as **logits**) for each of the 10 possible classes that the image could belong to. We can then **classify** the image as belonging to the class which receives the highest score, or we can quantify the model's \"confidence\" about the classifications by converting the scores into a probability distribution. \n",
    "\n",
    "A feed-forward neural network consisting of $N$ layers, applied to an input vector $\\mathbf{x}$ can be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{f_0} = \\mathbf{x} \\\\\n",
    "\\mathbf{f_i} = \\sigma_i(\\mathbf{W_if_{i-1}} + \\mathbf{b_i}) \\ \\ \\ i \\in [1, N]\n",
    "\\end{equation}\n",
    "\n",
    "Each layer has a particular number, $m_i$, of neurons. The parameters of a layer consist of a matrix $\\mathbf{W_i} \\in \\mathbb{R}^{m_i \\times m_{i-1}}$ and bias vector $\\mathbf{b_i} \\in \\mathbb{R}^{m_i}$. Each layer also has a non-linear activation function $\\sigma_i$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVdZG4dvDxjr"
   },
   "source": [
    "### Optional extra reading: Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ2U6G-SDxjs"
   },
   "source": [
    "\n",
    "\n",
    "Activation functions are a core ingredient in deep neural networks. In fact they are what allows us to make use of multiple layers in a neural network. There are a number of different activation functions, each of which are more or less useful under different circumstances. The four activation functions that you are most likely to encounter are, arguably, [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU), [Tanh](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh), [Sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid), and [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax). \n",
    "\n",
    "ReLU, has in recent years, become the default choice for use in MLPs and Convolutional Neural Networks (CNNs). ReLU has two advantages over Tanh and Sigmoid: it is computationally much more efficient, and, it allows us to use deeper networks because it does not suffer from [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). As a result of their success, a number of ReLU variants, such as [LeakyRelu](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LeakyReLU) and [PReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU), have been developed.\n",
    "\n",
    "Sigmoid and Softmax activations are often found after the last layer in binary and multi-class classification networks, respectively, as they transform the outputs of the network in such a way that we can interpret them as class probabilities.\n",
    "\n",
    "Both Tanh and Sigmoid are found in LSTM and GRU recurrent neural networks, which we will find out more about in the coming days. They are also useful in MLPs and CNNs where we want the output to be bounded between -1 and 1 (Tanh) or 0 and 1 (Sigmoid).\n",
    "\n",
    "Read more about activation functions [here](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSRNRokNDxjw"
   },
   "source": [
    "### Configure the feed-forward neural network\n",
    "We configure the feed-forward neural network part of our classifier using the [Keras Layers API](https://www.tensorflow.org/api_docs/python/tf/keras/layers). This API consists of various reusable building-blocks that allow us to define many different neural network architectures (similar to how we defined a data pipeline earlier!). \n",
    "\n",
    "Here we use the [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) component which allows us to wrap together a sequence of layers. An important point to note here is that we are **configuring** our neural network architecture as a pipeline. We can think of the resulting ```model``` variable as a *function* that takes a batch of images as inputs and outputs a batch of logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AQ_VBthDxjy"
   },
   "outputs": [],
   "source": [
    "# RE-RUN THIS CELL if you want to restart training!\n",
    "model = tf.keras.Sequential([\n",
    "    # Convert the 28x28 image into a flat vector of 28x28 = 784 values\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='flatten_input'),\n",
    "    # Create a \"hidden\" layer with 256 neurons and apply the ReLU non-linearity\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu, name='input_to_hidden1'),\n",
    "    # Create another hidden layer with 128 neurons\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu, name='hidden1_to_hidden2'),\n",
    "    # Create an \"output layer\" with 10 neurons\n",
    "    tf.keras.layers.Dense(10, name='hidden_to_logits'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YNP8uM3Dxj1"
   },
   "source": [
    "The following summary shows how many parameters each layer is made up of (the number of entries in the weight matrics and bias vectors). Note that a value of ```None``` in a particular dimension of a shape means that the shape will dynamically adapt based on the shape of the inputs. In particular, the output shape of the ```flatten_input``` layer will be $[N, 784]$ when the batch of inputs passed to the model has shape $[N, 28, 28]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L1WmzZdDxj2",
    "outputId": "aa41c3b8-0fbf-45df-efae-7f2e7767e00a"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiCUzgdpDxj4"
   },
   "source": [
    "**QUESTION**: Note that the flattening operation has no parameters. However, the Dense (fully-connected) layer mapping from the 784-D output of the flatten operation to the 256-D hidden layer has 200,960 parameters! Can you explain why the dense layer has 200,960 parameters? **HINT**: Refer back to the equations of a feed-forward network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baRyOEMXDxj5"
   },
   "source": [
    "### Define the loss\n",
    "As we did in the previous practical, we need to specify a loss function for our classifier. This tells us how good our model's predictions are compared to the actual labels (the targets), with a lower loss meaning better predictions. The standard loss function to use with a **multi-class classifier** is the **cross-entropy loss** also known as the \"negative log likelihood\" for a classifier. Suppose we have a classification problem with $C$ classes. A classifier is trained to predict a probability distribution $p(y | X_i)$ for each input $X_i$ from a batch of $N$ examples. The vector $p(y|X_i)$ is $C$ dimensional, sums to one, and we use $p(y|X_i)_c$ to denote the $c$th component of  $p(y|X_i)$. The true class for example $i$ in the batch is $y_i$ and we define the indicator function $\\mathbb{1}[y_i=c]$ to be 1 whenever $y_i = c$ and $0$ otherwise. This classifier has a cross-entropy loss of\n",
    "\n",
    "$- \\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C log( p(y|X_i)_c) \\mathbb{1}[y_i=c]$\n",
    "\n",
    "**NOTE**: The indicator is one for the true class label, and zero everywhere else. So in that sum, the indicator just \"lifts out\" the $log(p(y|X_i))$ values for all true classes. So the above expression is minimised (note the negative at the front) when the model places all its probability mass on the true labels for all the examples. Remember that  log(1)=0 , thus the closer all probabilities of $y_i = c$ are to one, the lower the loss will be and the better the model will be performing.\n",
    "\n",
    "\n",
    "Fortunately we don't need to write this function ourselves as Tensorflow provides a version called \n",
    "\n",
    "```tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)```. \n",
    "\n",
    "**NOTE**: This function actually computes the cross entropy loss directly from the un-normalised logits, rather than from the probability distribution for numerical stability.\n",
    "\n",
    "By the way, for training data in which the labels are themselves distributions rather than exact values, this definition of cross-entropy still works, where the indicator function is replaced with the corresponding probability of each class for that example. This might be important when we are not sure whether the training data has been labelled correctly, or when the data was labelled by a human who gave their answer along with a degree of confidence that the answer was correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzjQV7XRDxj6"
   },
   "source": [
    "## Train the model\n",
    "Now that we have our data, data processing pipeline, our neural network architecture and the corresponding loss that we want to minimise, we need to **train** the model using batched stochastic gradient descent. We do this in multiple **epochs**, which is a single iteration through the entire training dataset. Briefly, in each epoch we loop over all the batches of images and labels, and for each batch we perform the following steps:\n",
    "* Get the **predictions** of the model on the current batch of images\n",
    "* Compute the **average loss** values across the batch, telling us how good these predictions are / how close they are to the true targets.\n",
    "* Compute the **gradient of the average loss** (or the average gradient of the losses in the batch) with respect to each of the model's parameters: This tells us the direction to move in \"parameter space\" to decrease the loss value\n",
    "* **Adjust the parameters** by taking a small step in the direction of each component of the gradient (where the learning rate controls the size of the step)\n",
    "\n",
    "During training we also track some metrics, such as the loss and accuracy to see how well the classifier is doing. Note that the cell below may take a few minutes to run! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf9x20I3Dxj6"
   },
   "source": [
    "### Optional extra reading: Computing gradients in Eager mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb3A5nzqDxj7"
   },
   "source": [
    "In graph mode, TensorFlow builds up a \"computation graph\" which captures all operations of the model and their dependencies. For training, the gradient can then be computed by traversing backwards from every node through its dependents and applying the \"chain rule\" of differentiation. \n",
    "\n",
    "However, in Eager mode, we don't have the concept of the computation graph anymore. Operations are performed imperatively (in the order in which they were executed). TensorFlow therefore uses a mechanism called the \"GradientTape\" for computing gradients in Eager mode. Basically, the gradient tape records the order of all operations as they are executed, and can then be \"run backwards\" (traversed from last to first operation) for computing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYtxtqMuDxj8"
   },
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NITPuzu8Dxj9",
    "outputId": "df2069da-9385-41e5-f0f1-ea45f5fa0c58"
   },
   "outputs": [],
   "source": [
    "# Choose an optimizer and loss function for training:\n",
    "# The optimizer is responsible for controlling the learning rate\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Select metrics to measure the loss and the accuracy of the model. \n",
    "# These metrics accumulate the values over epochs and then print the overall result.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "num_epochs = 50  # The number of epochs to run\n",
    "\n",
    "# Lists to store the loss and accuracy of every epoch\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "# Defined a function to train the model using tf.GradientTape\n",
    "@tf.function\n",
    "def train_step(image, label):\n",
    "  # Initialise a GradientTape to track the operations\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Compute the logits (un-normalised scores) of the current batch of examples\n",
    "    # using the neural network architecture we defined earlier\n",
    "    logits = model(image)\n",
    "    loss = loss_object(label, logits)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  # Add current batch loss to our loss metric tracker - note the function call semantics\n",
    "  train_loss(loss)\n",
    "  train_accuracy(label, logits)\n",
    "\n",
    "# Define a function to test the model\n",
    "# @tf.function\n",
    "# def val_step(image, label):\n",
    "  # TODO\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # Loop over our data pipeline\n",
    "  for image, label in train_ds:\n",
    "    train_step(image, label)\n",
    "      \n",
    "  template = 'Epoch {:03d}, Loss: {:.3f}, Accuracy: {:.3%}'\n",
    "  print(template.format(epoch+1,\n",
    "                        train_loss.result(), \n",
    "                        train_accuracy.result()))\n",
    "  \n",
    "  train_losses.append(train_loss.result())\n",
    "  train_accuracies.append(train_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "hfMeioguDxkB",
    "outputId": "c97e3769-5a21-4f1d-f342-f3a52f3251ed"
   },
   "outputs": [],
   "source": [
    "# Plot the loss for all epochs using Matplotlib\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_losses)\n",
    "plt.title('Loss vs epochs')\n",
    "\n",
    "# Plot the accuracy for all epochs using Matplotlib\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_accuracies)\n",
    "plt.title('Accuracy vs epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrQfWEH3DxkG"
   },
   "source": [
    "The above code block shows a typical training loop. There's actually an easier way to do this using Tenosrflow and Keras, which we'll see in the next prac, but for now we expand it so you can see what's going on.\n",
    "\n",
    "Lets visualise some of the model's prediction on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724
    },
    "id": "YdV_nzhlDxkH",
    "outputId": "acd841e8-c17c-49df-f33a-0b26a6cb7900"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_ds))  # Get a batch of images and labels\n",
    "\n",
    "_logits = model(images, training=False)  # Pass the images to the model function and get its output logits\n",
    "predicted_labels = tf.argmax(_logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "img_indexs = np.arange(images.numpy().shape[0])\n",
    "np.random.shuffle(img_indexs)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = img_indexs[i]\n",
    "    predicted_label = int(predicted_labels[img_index])\n",
    "\n",
    "    plt.imshow(images[img_index], cmap=\"gray_r\")\n",
    "\n",
    "    actual_label = int(labels[img_index].numpy())\n",
    "    plt.xlabel(\"Actual: {} ({})\\n Predicted: {} ({})\".format(\n",
    "        actual_label, text_labels[actual_label], predicted_label, text_labels[predicted_label]\n",
    "    ))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGlo8xJYDxkJ"
   },
   "source": [
    " ### Optional extra reading: Optimisation schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97dXX396DxkK"
   },
   "source": [
    "You might have noticed that we are using the Adam optimizer to train our neural networks. Adam is a variant of stochastic gradient descent which often performs well in practice. In fact, there is a whole range of variations on stochastic gradient descent that are often used. Here is an illustration showing how a few of these methods perform on a toy problem: \n",
    "\n",
    "![optimization methods](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)\n",
    "\n",
    "For a detailed description of various optimization methods read [this](http://ruder.io/optimizing-gradient-descent/) article. For a great visual discussion on how these methods work see [this](https://distill.pub/2017/momentum/) article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du-C5ktMDxkL"
   },
   "source": [
    "## Testing\n",
    "\n",
    "After 50 epochs of training on our training set, we obtain a loss of around 0.061 and accuracy of 97.7%. That's pretty good, right? \n",
    "\n",
    "It is important to distinguish between data that is **in-sample** and **out-of-sample**. Our training data is all in-sample and we would expect any resonably powerful model (like an MLP) to be very accurate and have a low loss on this (in fact, with a sufficiently large MLP and enough training epochs, we can get the loss arbitrarily close to zero!). This is not a good thing though, because pushing the loss close to zero may mean that the model has fit the **noise** in the training data rather than the true signal. If this is the case, it will not perform well on out-of-sample data that it has not seen before (which is what we really care about!). To assess this, we evaluate our trained model on the held-out test set (but we don't update the parameters of the model when testing it, that would be cheating!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yy2dJWoDxkL"
   },
   "outputs": [],
   "source": [
    "# We need to apply the same pre-processing to the test set as we did to the training set\n",
    "# Since we don't need batching or shuffling, we can do this directly instead of\n",
    "# building a tf.Dataset pipeline\n",
    "\n",
    "tf_test_images = tf.convert_to_tensor(test_images, dtype=tf.float32) / 255.0\n",
    "tf_test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7WnRsTiDxkN",
    "outputId": "67269843-b50b-4e81-c94b-6cce4b6d31dc"
   },
   "outputs": [],
   "source": [
    "test_logits = model(tf_test_images, training=False)\n",
    "\n",
    "# Compute the average cross-entropy loss of the classification over the entire test set\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "t_loss = loss_object(tf_test_labels, test_logits)\n",
    "test_loss(t_loss)\n",
    "\n",
    "# Compare predicted labels to actual labels\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "test_accuracy(tf_test_labels, test_logits)\n",
    "\n",
    "print('Completed testing on', tf_test_images.shape[0], 'examples...')\n",
    "print('Loss: {:.3f}, Accuracy: {:.3%}'.format(test_loss.result(), test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY868WQTDxkP"
   },
   "source": [
    "And again we visualise some of the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "id": "qMlYbWWyDxkQ",
    "outputId": "24c3dc66-e807-46eb-81cd-fc55a5a9b58a"
   },
   "outputs": [],
   "source": [
    "test_predictions = tf.argmax(test_logits, axis=1, output_type=tf.int32)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    img_index = np.random.randint(0, 10000)\n",
    "    plt.imshow(test_images[img_index], cmap=\"gray_r\")\n",
    "\n",
    "    actual_label = int(test_labels[img_index])\n",
    "    predicted_label = int(test_predictions[img_index])\n",
    "\n",
    "    plt.xlabel(\"Actual: {} ({})\\n Predicted: {} ({})\".format(\n",
    "        actual_label, text_labels[actual_label], predicted_label, text_labels[predicted_label]\n",
    "    ))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TphywfV4DxkU"
   },
   "source": [
    "What happened? We got 97.7% accuracy on the training set, but only 88.9% on the test set. This is an example of how a model's in-sample performance differs from its out-of-sample performance. It may even be considered **overfitting** (where the model failing to generalise its in-sample performance to unseen out-of-sample data). There are a number of ways we can address this, but first, it's important to realise that we should try and keep the number of evaluations on the test set to a minimum. \n",
    "\n",
    "\n",
    "The first important method is called **validation**. Using validation, we reserve a portion of our training dataset and call it the validation set. After each epoch of training (or every K epochs where K can be a fraction), we evaluate our model on this validation set (without updating the model parameters). This gives us a good sense of how well we expect our model to do on out-of-sample data. In this way, the validation set acts as a surrogate for the test set which we can safely use during model development and training. We can also use this validation set to tune our model by selecting hyperparameters such as the batch size, sizes of layers or the network architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMvjNMKkDxkX"
   },
   "source": [
    "### Optional extra reading: Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKq_JYnxDxkZ"
   },
   "source": [
    "In addition to the problem of _overfitting_ (in which the model is too complex and as a result is able to memorize the training data rather than learning a general pattern) there is the problem of _underfitting_ which is the opposite. Underfitting occurs when the model does not have enough complexity (also known as capacity) to learn a general pattern. While overfitting is characterised by diverging test/validation and training scores, underfitting is characterised by test/validation and training scores that very slowly continue to improve. \n",
    "\n",
    "\n",
    "Unlike overfitting which has a number of solutions, such as collecting more data, data augmentation, dropout, and L1/L2 regularization, underfitting has one simple solution: just make your model incrementally more complex until it no longer underfits.\n",
    "\n",
    "![underandoverfitting](https://i.imgur.com/m2bSP1S.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aCRfDi_lDxjA",
    "baRyOEMXDxj5"
   ],
   "name": "Copy of Feedforward Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
